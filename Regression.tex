\documentclass[main.tex]{subfiles}
\begin{document}

\paragraph{Регрессионный анализ}
% 38
\begin{definition}[Регрессия] ~\\
	Пусть $Y$ - наблюдение,  $Z$ - характеристика, определяющая распределение $Y$, $F_Z$ - распределение $Y$ при фиксированном $Z$.\\
	Пусть $Y_1,\dots,Y_n$ - независимы.
	Установим зависимость $Y_i$ от $i$.
	Сопоставим $\forall i: i \mapsto Z_i \implies F_i \equiv F_{Z_i}$.
	Обычно эту зависимость задают параметрически ($\ex: F_i = g_\theta(F_{Z_0}), \theta \in \R^d$).\\
	Тогда $E_\theta(Y|Z) = g_\theta(Z)$ - \textbf{регрессия} $Y$ по $Z$.
\end{definition}

\begin{definition}[Линейная регрессия] ~\\
	Регрессия называется \textbf{линейной} если
	\[\exists X(Z)=
		\begin{pmatrix}
		X_1(Z)\\
		\vdots\\
		X_n(Z)
		\end{pmatrix}
		\text{ - регрессор.} 
	\]
	\textbf{Модель линейной регрессии}
	\[\sE_\theta(Y|Z) = X^T\beta,\ \beta =
		\begin{pmatrix}
			\beta_1\\
			\vdots\\
			\beta_n
		\end{pmatrix}
	\]
	В условиях этой модели
	\begin{align*}
		\sE_\theta Y_i &= (X(Z_i))^T\beta\\
		Y_i &= (X(Z_i))^T\beta + \varepsilon_i\\
		Y &= X^T\beta + \varepsilon (\sE\varepsilon = 0)\\
	\end{align*}
	Где $X \in M_{m\times n}$ - матрица регрессоров, $\beta$ - $m \times 1$ - столбец параметров, $\varepsilon = (\varepsilon_1,\dots,\varepsilon_n)^T\text{ - вектор отклонений.}$
\end{definition}
\textbf{Примеры регрессионных моделей.}
$Y_1,\dots,Y_n$ - независимые наблюдения.
\begin{enumerate}
	\item Выборка
	\[\sE Y_i = \beta_i \text{ Если $\varepsilon_i$ - НОРСВ, то } Y_1,\dots,Y_n\]
	\item Простая регрессионная модель
	\[Y_i = \beta_1 + \beta_2 Z_i + \varepsilon_i,\ X = \begin{pmatrix}
		1   & \cdots & 1\\
		Z_1 & \cdots & Z_n\\
	\end{pmatrix}\]
	\item Полиномиальная модель
	\[Y_i = \sum_{j=1}^s \beta_jZ_i^{j-1},\ X = \begin{pmatrix}
		1 & \cdots & 1\\
		Z_1 & \cdots & Z_n\\
		Z_1^2 & \cdots & Z_n^2\\
		\vdots & \ddots & \vdots\\
		Z_1^{s-1} & \cdots & Z_n^{s-1}
	\end{pmatrix}\]
	\item Простая группировка (однофакторный дисперсионный анализ)
	Пусть $Z \in \{1,\dots,I\}$, $\beta = (\beta_1,\dots,\beta_I)^T$, тогда
	\begin{align*}
		&\sE(Y|Z) = \beta_Z\\
		&Y_i = \beta_{Z_i} + \varepsilon_i\\
		&\forall i < j: Z_i <= Z_j\\
		&X = \begin{pmatrix}
			1 & 1 & 0 & 0 & & & &\\
			0 & 0 & 1 & 1 & &  \textbf{0} & &\\
			0 & 0 & 0 & 0 & & & &\\
			& & & & \ddots & & &\\
			& & \textbf{0} & & & \ddots & &\\
			& & & & & & 1 & 1
		\end{pmatrix}
	\end{align*}
\end{enumerate}
% 39
\begin{definition}[Метод наименьших квадратов]~\\
Пусть $Y = X^T\beta + \varepsilon$.\\
Рассмотрим $S(\beta) = ||Y-X^T\beta||^2=(Y-X^T\beta)^T(Y-X^T\beta) = \sum_{i=1}^n(Y_i-\sum_{j=1}^mx_{ji}\beta_j)^2$.\\
Тогда $\hat \beta$ называется МНК-оценкой $\beta$, если $\forall \beta: S(\hat \beta)<=S(\beta)$.
\end{definition}
МНК-оценка $\hat \beta$ находится как решение системы уравнений
\[\begin{dcases}
		\frac{\partial S}{\partial \beta_k}\bigg(-2\sum_{i=1}^nx_{ki}(Y_i-\sum_{j=1}^mx_{ji}\beta_j)^2\bigg) = 0\\
		k \in (1,\dots,m)
\end{dcases}\]
\[\begin{dcases}
		\sum_{i=1}^nx_{ki}Y_i=\sum_{i=1}^nx_{ki}\sum_{j=1}^mx_{ji}\beta_j = 0\\
		k \in (1,\dots,m)
\end{dcases}\]
Или в матричной форме\\
$XX^T\beta=XY$ - система нормальных уравнений\\
$\hat\beta$ - решение системы, МНК-оценка\\
$|XX^T| \neq 0 \implies \hat\beta = (XX^T)^{-1}XY$, т.е. $\exists!$ решение.\\
\\
\textbf{Геометрическая интерпретация}\\
\[	Y = \sum_{i=1}^m X_i\beta_i + \varepsilon\text{, где }X_j = \begin{pmatrix}
	x_{j1}\\
	\vdots\\
	x_{jn}
\end{pmatrix}}\text{ - транспонированные строки X.}\]
Пусть $Y \in V_n$ - линейное пространство наблюдений, $V_r = \alpha(X_1,\dots,X_m)$  - линейное подпространство на $(X_1,\dots,X_m)$, а $r = rk(x)$ - размерность пространства.\\
\begin{align*}
\text{Тогда } &||Y-X^T\beta|| \text{- расстояние от $Y$ до точки  в $V_r$}\\
&||Y-X^T\beta||\rightarrow \min\text{, если } \eta = X^T\beta=\proj_{V_r}Y\\
\text{При этом } & X_k \perp (Y-X^T\beta), \forall k \in \{1\ldots m\}\\
\iff & X_k^T(Y-X^T\beta) = 0, \forall k \in \{1\ldots m\}\\
\iff & X(Y-X^T\beta) = 0\\
\iff & XX^T\beta = XY\\
\implies &\text{ существует единственное решение.}
\end{align*}
% 40
\textbf{ДНО функции параметра.} ~\\
\begin{definition}[Функция параметра] ~\\
	Мы будем называть $\psi$ функцией параметра $\beta$, если
	\[\psi = C^T\beta,\,(\psi = (\psi_1,\dots,\psi_q)^T)\]
\end{definition}
\begin{definition}[Линейная оценка] ~\\
	Мы будем называть $\hat \psi$ линейной оценкой, если
	\[\hat\psi = AY\ (A \in M_{q \times n})\]
\end{definition}
\begin{definition}[ДНО функция параметра] ~\\
	Мы будем называть функцию параметра $\psi = C^T\beta$ допускающей несмещённое оценивание (ДНО), если
	\[\exists \hat \psi = AY: \sE_\theta \hat \psi = \psi,\,\forall \theta \in \Theta = \R^m \times \R\]
\end{definition}
\begin{example} ~\\
	Если $|XX^T| \neq 0$, $\hat\beta = (XX^T)^{-1}XY$, тогда
	\begin{align*}
		\sE_{\hat\beta} = (XX^T)^{-1}X\sEY &= (XX^T)^{-1}(XX^T)\beta = \beta\\
		\implies \sE_\theta\hat\psi &= \psi, \forall\psi = C^T\beta
	\end{align*}
\end{example}
\begin{statement} ~\\
	Функция параметра $\psi = C^T\beta$ - ДНО $\iff \exists A: C^T = AX^T$
\end{statement}
\begin{theorem}[Гаусса-Маркова] ~\\
	Пусть $\psi$ - ДНО функция параметра, $\psi = C^T\beta, q = 1$. Тогда существует $\hat\psi$ - линейная несмещённая оценка, такая что $\hat\psi$ - НРМД оценка.\\
	При этом для любых $\hat\beta$ решений системы нормальных уравнений $\hat\psi=C^T\hat\beta$
\end{theorem}
\end{document}
