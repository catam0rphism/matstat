\documentclass[main.tex]{subfiles}
\begin{document}

\paragraph{Регрессионный анализ}
% 38
\begin{definition}[Регрессия] ~\\
	Пусть $Y$ - наблюдение,  $Z$ - характеристика, определяющая распределение $Y$, $F_Z$ - распределение $Y$ при фиксированном $Z$.\\
	Пусть $Y_1,\dots,Y_n$ - независимы.
	Установим зависимость $Y_i$ от $i$.
	Сопоставим $\forall i: i \mapsto Z_i \implies F_i \equiv F_{Z_i}$.
	Обычно эту зависимость задают параметрически ($\ex: F_i = g_\theta(F_{Z_0}), \theta \in \R^d$).\\
	Тогда $E_\theta(Y|Z) = g_\theta(Z)$ - \textbf{регрессия} $Y$ по $Z$.
\end{definition}

\begin{definition}[Линейная регрессия] ~\\
	Регрессия называется \textbf{линейной} если
	\[\exists X(Z)=
		\begin{pmatrix}
		X_1(Z)\\
		\vdots\\
		X_n(Z)
		\end{pmatrix}
		\text{ - регрессор.} 
	\]
	\textbf{Модель линейной регрессии}
	\[\sE_\theta(Y|Z) = X^T\beta,\ \beta =
		\begin{pmatrix}
			\beta_1\\
			\vdots\\
			\beta_n
		\end{pmatrix}
	\]
	В условиях этой модели
	\begin{align*}
		\sE_\theta Y_i &= (X(Z_i))^T\beta\\
		Y_i &= (X(Z_i))^T\beta + \varepsilon_i\\
		Y &= X^T\beta + \varepsilon (\sE\varepsilon = 0)\\
	\end{align*}
	Где $X \in M_{m\times n}$ - матрица регрессоров, $\beta$ - $m \times 1$ - столбец параметров, $\varepsilon = (\varepsilon_1,\dots,\varepsilon_n)^T\text{ - вектор отклонений.}$
\end{definition}
\textbf{Примеры регрессионных моделей.} ~\\
$Y_1,\dots,Y_n$ - независимые наблюдения.
\begin{enumerate}
	\item Выборка
	\[\sE Y_i = \beta_i \text{ Если $\varepsilon_i$ - НОРСВ, то } Y_1,\dots,Y_n\]
	\item Простая регрессионная модель
	\[Y_i = \beta_1 + \beta_2 Z_i + \varepsilon_i,\ X = \begin{pmatrix}
		1   & \cdots & 1\\
		Z_1 & \cdots & Z_n\\
	\end{pmatrix}\]
	\item Полиномиальная модель
	\[Y_i = \sum_{j=1}^s \beta_jZ_i^{j-1},\ X = \begin{pmatrix}
		1 & \cdots & 1\\
		Z_1 & \cdots & Z_n\\
		Z_1^2 & \cdots & Z_n^2\\
		\vdots & \ddots & \vdots\\
		Z_1^{s-1} & \cdots & Z_n^{s-1}
	\end{pmatrix}\]
	\item Простая группировка (однофакторный дисперсионный анализ)
	Пусть $Z \in \{1,\dots,I\}$, $\beta = (\beta_1,\dots,\beta_I)^T$, тогда
	\begin{align*}
		&\sE(Y|Z) = \beta_Z\\
		&Y_i = \beta_{Z_i} + \varepsilon_i\\
		&\forall i < j: Z_i <= Z_j\\
		&X = \begin{pmatrix}
			1 & 1 & 0 & 0 & & & &\\
			0 & 0 & 1 & 1 & &  \textbf{0} & &\\
			0 & 0 & 0 & 0 & & & &\\
			& & & & \ddots & & &\\
			& & \textbf{0} & & & \ddots & &\\
			& & & & & & 1 & 1
		\end{pmatrix}
	\end{align*}
\end{enumerate}
% 39
\begin{definition}[Метод наименьших квадратов]~\\
Пусть $Y = X^T\beta + \varepsilon$.\\
Рассмотрим $S(\beta) = ||Y-X^T\beta||^2=(Y-X^T\beta)^T(Y-X^T\beta) = \sum_{i=1}^n(Y_i-\sum_{j=1}^mx_{ji}\beta_j)^2$.\\
Тогда $\hat \beta$ называется МНК-оценкой $\beta$, если $\forall \beta: S(\hat \beta)<=S(\beta)$.
\end{definition}
МНК-оценка $\hat \beta$ находится как решение системы уравнений
\[\begin{dcases}
		\frac{\partial S}{\partial \beta_k}\bigg(-2\sum_{i=1}^nx_{ki}(Y_i-\sum_{j=1}^mx_{ji}\beta_j)^2\bigg) = 0\\
		k \in (1,\dots,m)
\end{dcases}\]
\[\begin{dcases}
		\sum_{i=1}^nx_{ki}Y_i=\sum_{i=1}^nx_{ki}\sum_{j=1}^mx_{ji}\beta_j = 0\\
		k \in (1,\dots,m)
\end{dcases}\]
Или в матричной форме\\
$XX^T\beta=XY$ - система нормальных уравнений\\
$\hat\beta$ - решение системы, МНК-оценка\\
$|XX^T| \neq 0 \implies \hat\beta = (XX^T)^{-1}XY$, т.е. $\exists!$ решение.\\
\\
\textbf{Геометрическая интерпретация}\\
\[	Y = \sum_{i=1}^m X_i\beta_i + \varepsilon\text{, где }X_j = \begin{pmatrix}
	x_{j1}\\
	\vdots\\
	x_{jn}
\end{pmatrix}\text{ - транспонированные строки X.}\]
Пусть $Y \in V_n$ - линейное пространство наблюдений, $V_r = \alpha(X_1,\dots,X_m)$  - линейное подпространство на $(X_1,\dots,X_m)$, а $r = rk(x)$ - размерность пространства.\\
\begin{align*}
\text{Тогда } &||Y-X^T\beta|| \text{- расстояние от $Y$ до точки  в $V_r$}\\
&||Y-X^T\beta||\rightarrow \min\text{, если } \eta = X^T\beta=\proj_{V_r}Y\\
\text{При этом } & X_k \perp (Y-X^T\beta), \forall k \in \{1\ldots m\}\\
\iff & X_k^T(Y-X^T\beta) = 0, \forall k \in \{1\ldots m\}\\
\iff & X(Y-X^T\beta) = 0\\
\iff & XX^T\beta = XY\\
\implies &\text{ существует единственное решение.}
\end{align*}
% 40
\\
\textbf{ДНО функции параметра.}	
\begin{definition}[Функция параметра] ~\\
	Мы будем называть $\psi$ функцией параметра $\beta$, если
	\[\psi = C^T\beta,\,(\psi = (\psi_1,\dots,\psi_q)^T)\]
\end{definition}
\begin{definition}[Линейная оценка] ~\\
	Мы будем называть $\hat \psi$ линейной оценкой, если
	\[\hat\psi = AY\ (A \in M_{q \times n})\]
\end{definition}
\begin{definition}[ДНО функция параметра] ~\\
	Мы будем называть функцию параметра $\psi = C^T\beta$ допускающей несмещённое оценивание (ДНО), если
	\[\exists \hat \psi = AY: \sE_\theta \hat \psi = \psi,\,\forall \theta \in \Theta = \R^m \times \R\]
\end{definition}
\begin{example} ~\\
	Если $|XX^T| \neq 0$, $\hat\beta = (XX^T)^{-1}XY$, тогда
	\begin{align*}
		\sE_{\hat\beta} = (XX^T)^{-1}X\sE Y &= (XX^T)^{-1}(XX^T)\beta = \beta\\
		\implies \sE_\theta\hat\psi &= \psi, \forall\psi = C^T\beta
	\end{align*}
\end{example}
\begin{statement} ~\\
	Функция параметра $\psi = C^T\beta$ - ДНО $\iff \exists A: C^T = AX^T$
\end{statement}
\begin{theorem}[Гаусса-Маркова] ~\\
	Пусть $\psi$ - ДНО функция параметра, $\psi = C^T\beta, q = 1$. Тогда существует $\hat\psi$ - линейная несмещённая оценка, такая что $\hat\psi$ - НРМД оценка.\\
	При этом для любых $\hat\beta$ решений системы нормальных уравнений $\hat\psi=C^T\hat\beta$
\end{theorem}
~\\
% 41
\textbf{Доверительное оценивание параметра.} ~\\
Пусть $Y = X^T\beta + \varepsilon$, где $Y \in \R^n$ - вектор наблюдений, $X \in M_{m \times n}$ - матрица регрессоров, $\beta \in \R^m$ - вектор параметров, а $\varepsilon \in \R^n$ - вектор отклонений (ошибок).
\begin{suggestion} ~\\
	$\varepsilon \sim \sN(0, \sigma^2n),\ ((\varepsilon_1,\dots,\varepsilon_n) \text{ - НОРСВ } \sN(0,\sigma^2))$\\
	Тогда $Y \sim \sN(X^T\beta, \sigma^2n),\ (Y_1,\dots,Y_n)$ - НСВ.\\
	Пусть $\psi = C^T\beta$ - ДНО, а $\hat\psi=C^T\hat\beta$ - НРМД оценка.
\end{suggestion}
\begin{statement} ~\\
	В данных условиях:
	\begin{enumerate}
		\item $\hat\psi \sim \sN(\psi, \Gamma(\psi))$
		\item $\hat\psi,s^2$ - независимы.
		\item $\cfrac{(n-r)s^2}{\sigma^2} \sim \bigchi_{n-r}^2$
	\end{enumerate}
	Известный факт:\\
	Для $\xi = (\xi_1,\cdots,\xi_q)$, таких что $\xi \sim \sN(0,\Sigma),\ |\Sigma| \neq 0$
	\[\xi^T\Sigma^{-1}\xi \sim \bigchi_q^2\]
	Тогда:
	\begin{align*}
		(1) &\implies (\hat\psi-\psi)^T\Gamma^{-1}_{\hat\psi}(\hat\psi-\psi) \sim \bigchi_q^2\\
		(3) &\implies \cfrac{(n-r)s^2}{\sigma^2} \sim \bigchi_{n-r}^2\\
		(2) &\implies \cfrac{(\hat\psi-\psi)^T\Gamma^{-1}_{\hat\psi}(\hat\psi-\psi)}{q \cfrac{s^2}{\sigma^2}} \sim F_{q,n-r}\\
			&\implies \Gamma^{-1}_{\hat\psi} = (B\sigma^2)^{-1} = \cfrac{B^{-1}}{\sigma^2}, B = C^TS^-C
	\end{align*}
\end{statement}
\begin{definition}[Доверительный эллипсоид] ~\\
Пусть $\hat\Theta = \{\psi \mid (\hat\psi - \psi)^TB^{-1}(\hat\psi - \psi) \leq qs^2x_\alpha\}$\\
Где $x_\alpha = F^{-1}_{q,n-r}(1-\alpha)$, то есть $F_{1,n-r}(x_\alpha)=1-\alpha$\\
Тогда $P_\theta(\psi \in \hat\Theta) = 1-\alpha, \forall \theta \in \Theta$\\
Таким образом $\hat\Theta$ - доверительный эллипсоид уровня доверия $1-\alpha$\\
\textbf{Частный случай: $q=1$}
\begin{gather*}
	\hat\psi \sim \sN(\psi,b\sigma^2),\ (b\sigma^2=\mathrm{D}\hat\psi)\\
	\cfrac{\hat\psi-\psi}{\sqrt{b}\sigma}\sim\sN(0,1),\ \cfrac{(n-r)s^2}{\sigma^2}\sim\bigchi_{n-r}^2\\
	\implies \cfrac{(\hat\psi-\psi)/\sqrt{b}\sigma}{\sqrt{\cfrac{(n-r)s^2}{(n-r)\sigma^2}}} = \cfrac{\hat\psi-\psi}{\sqrt{b}s} \sim S_{n-r}\\
	t_\alpha:S_{n-r}(t_\alpha) = 1 * \cfrac{\alpha}{2}
\end{gather*}
Тогда
\begin{gather*}
	P_\theta(-t_\alpha \leq \cfrac{\hat\psi-\psi}{\sqrt{b}s} \leq t_\alpha) = 1 - \alpha, \forall \theta\\
	P_\theta(\hat\psi - t_\alpha\sqrt{b}s \leq \psi \leq \hat\psi + t_\alpha\sqrt{b}s)\\
\end{gather*}
И $\hat\psi \pminus t_\alpha\sqrt{b}s$ - доверительный интервал уровня доверия $1 - \alpha$
\end{definition}
\end{document}
