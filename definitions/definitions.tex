\documentclass[titlepage]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[english, russian]{babel} % Localisation
% \usepackage{pscyr} % fonts

% \usepackage{fontspec} % loaded by polyglossia, but included here for transparency 
% \usepackage{polyglossia}
% \setmainlanguage{russian} 
% \setotherlanguage{english}

\usepackage{amssymb} % More math symbols
\usepackage{amsmath} % Math constructions
\usepackage{amsthm} % Theorems
\usepackage{subfiles} % File separation
\usepackage{titlepic} % Logo =)
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{mathtools} % multiline equations with curly brace
\usepackage{mathrsfs}

\newcommand{\sP}{\mathcal{P}}
\newcommand{\sF}{\mathfrak{F}}
\newcommand{\sX}{\mathfrak{X}}
\newcommand{\sE}{\mathrm{E}}
\newcommand{\sD}{\mathrm{D}}
\newcommand{\sN}{\mathrm{N}} % N as N(a,s^2)
\newcommand{\R}{\mathbb{R}} % R as in Real
\newcommand{\N}{\mathbb{N}} % N as in Natural
\newcommand*{\bigchi}{\mbox{\Large$\chi$}} % Chi for chi-square
\DeclareMathOperator{\ex}{ex} % Example
\DeclareMathOperator{\proj}{Pr} % Projection

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{definition}{Определение}
\newtheorem*{example}{Пример}
\newtheorem*{statement}{Утверждение}
\newtheorem*{suggestion}{Предложение}
\newtheorem*{addition}{Дополнение}

\begin{document}
\paragraph{1. Что такое регрессия величины Y по величине X и что представляет собой модель линейной регрессии} % (fold) 
\begin{definition}[Регрессия] ~\\
	Пусть $Y$ - наблюдение,  $Z$ - характеристика, определяющая распределение $Y$, $F_Z$ - распределение $Y$ при фиксированном $Z$.\\
	Пусть $Y_1,\dots,Y_n$ - независимы.
	Установим зависимость $Y_i$ от $i$.
	Сопоставим $\forall i: i \mapsto Z_i \implies F_i \equiv F_{Z_i}$.
	Обычно эту зависимость задают параметрически ($\ex: F_i = g_\theta(F_{Z_0}), \theta \in \R^d$).\\
	Тогда $E_\theta(Y|Z) = g_\theta(Z)$ - \textbf{регрессия} $Y$ по $Z$.\\
	\textbf{Модель линейной регрессии}
	\[\sE_\theta(Y|Z) = X^T\beta,\ \beta =
		\begin{pmatrix}
			\beta_1\\
			\vdots\\
			\beta_n
		\end{pmatrix}
	\]
	В условиях этой модели
	\begin{align*}
		\sE_\theta Y_i &= (X(Z_i))^T\beta\\
		Y_i &= (X(Z_i))^T\beta + \varepsilon_i\\
		Y &= X^T\beta + \varepsilon (\sE\varepsilon = 0)\\
	\end{align*}
	Где $X \in M_{m\times n}$ - матрица регрессоров, $\beta$ - $m \times 1$ - столбец параметров, $\varepsilon = (\varepsilon_1,\dots,\varepsilon_n)^T\text{ - вектор отклонений.}$
\end{definition}

\paragraph{2. Что представляет собой оценка по методу наименьших квадратов}
\begin{definition}[Метод наименьших квадратов]~\\
	Пусть $Y = X^T\beta + \varepsilon$.\\
	Рассмотрим $S(\beta) = ||Y-X^T\beta||^2=(Y-X^T\beta)^T(Y-X^T\beta) = \sum_{i=1}^n(Y_i-\sum_{j=1}^mx_{ji}\beta_j)^2$.\\
	Тогда $\hat \beta$ называется МНК-оценкой $\beta$, если $\forall \beta: S(\hat \beta)<=S(\beta)$.\\
\end{definition}

\paragraph{3. Геометрический смысл метода наименьших квадратов}
\[	Y = \sum_{i=1}^m X_i\beta_i + \varepsilon\text{, где }X_j = \begin{pmatrix}
	x_{j1}\\
	\vdots\\
	x_{jn}
\end{pmatrix}\text{ - транспонированные строки X.}\]
Пусть $Y \in V_n$ - линейное пространство наблюдений, $V_r = \alpha(X_1,\dots,X_m)$  - линейное подпространство на $(X_1,\dots,X_m)$, а $r = rk(x)$ - размерность пространства.\\
\begin{align*}
	\text{Тогда } &||Y-X^T\beta|| \text{- расстояние от $Y$ до точки  в $V_r$}\\
	&||Y-X^T\beta||\rightarrow \min\text{, если } \eta = X^T\beta=\proj_{V_r}Y\\
	\text{При этом } & X_k \perp (Y-X^T\beta), \forall k \in \{1\ldots m\}\\
	\iff & X_k^T(Y-X^T\beta) = 0, \forall k \in \{1\ldots m\}\\
	\iff & X(Y-X^T\beta) = 0\\
	\iff & XX^T\beta = XY\\
	\implies &\text{ существует единственное решение.}
\end{align*}

\paragraph{4. Какие функции параметра модели линейной регрессии допускают несмещенное оценивание}
\begin{definition}[Функция параметра] ~\\
	Мы будем называть $\psi$ функцией параметра $\beta$, если
	\[\psi = C^T\beta,\,(\psi = (\psi_1,\dots,\psi_q)^T)\]
\end{definition}
\begin{definition}[Линейная оценка] ~\\
	Мы будем называть $\hat \psi$ линейной оценкой, если
	\[\hat\psi = AY\ (A \in M_{q \times n})\]
\end{definition}
\begin{definition}[ДНО функция параметра] ~\\
	Мы будем называть функцию параметра $\psi = C^T\beta$ допускающей несмещённое оценивание (ДНО), если
	\[\exists \hat \psi = AY: \sE_\theta \hat \psi = \psi,\,\forall \theta \in \Theta = \R^m \times \R\]
\end{definition}

\paragraph{5. Теорема Гаусса-Маркова}
\begin{theorem}[Гаусса-Маркова] ~\\
	Пусть $\psi$ - ДНО функция параметра, $\psi = C^T\beta, q = 1$. Тогда существует $\hat\psi$ - линейная несмещённая оценка, такая что $\hat\psi$ - НРМД оценка.\\
	При этом для любых $\hat\beta$ решений системы нормальных уравнений $\hat\psi=C^T\hat\beta$
\end{theorem}

\paragraph{6. Что такое нормальные уравнения} ~\\
МНК-оценка $\hat \beta$ находится как решение системы нормальных уравнений
\[\begin{dcases}
		\frac{\partial S}{\partial \beta_k}\bigg(-2\sum_{i=1}^nx_{ki}(Y_i-\sum_{j=1}^mx_{ji}\beta_j)^2\bigg) = 0\\
		k \in (1,\dots,m)
\end{dcases}\]
\[\begin{dcases}
		\sum_{i=1}^nx_{ki}Y_i=\sum_{i=1}^nx_{ki}\sum_{j=1}^mx_{ji}\beta_j = 0\\
		k \in (1,\dots,m)
\end{dcases}\]
Или в матричной форме\\
$XX^T\beta=XY$ - система\\
$\hat\beta$ - решение системы, МНК-оценка\\
$|XX^T| \neq 0 \implies \hat\beta = (XX^T)^{-1}XY$, т.е. $\exists!$ решение.\\

\paragraph{7. Определение порядковых статистик и рангов}
\begin{definition}[Порядковые статистики, вариационный ряд и ранги] ~\\
	Достаточная статистика
	$$X_{(1)}\leq X_{(2)}\leq \dots X_{(n)}$$ называется вариационным рядом, $X_(k)$ - $k$-ая порядковая статистика. Ранг $R_k$ - номер $x_k$ в вариационном ряду
\end{definition}

\paragraph{8. Что такое выборочная функция распределения}
\begin{definition}[Выборочная функция распределения] ~\\
	Выборочной функцией распределения называют $$F_n(x)=\frac{1}{n}\sum_{i=1}^n\mathbbm{1}_{\{X_i < x\}}=\frac{\text{Число наблюдений меньших }x}{\text{общее число наблюдений}}$$
\end{definition}

\paragraph{9. Что представляет собой случайная величина, функция распределения которой – выборочная функция распределения}

\paragraph{10. Теорема Гливенко-Кантелли}
\begin{theorem}[Гливенко-Кантелли]
	$$\sup_x \{|F_n(x)-F(x)|\} \xrightarrow[n\rightarrow \infty]{P_\theta=1} 0,\forall F$$
\end{theorem}
Утверждает сильную состоятельность $F_n$

\paragraph{11. Теорема Колмогорова}
\begin{theorem}[Колмогорова]
	$$\sqrt{n} \sup_x \{F_n(x)-F(x)\} \underset{F}{\Rightarrow}\mathcal{K}$$
\end{theorem}
Где $\mathcal{K}$ -  распределение Колмогорова, функция распределения ${\mathcal{K}(x) = \sum_{j=-\infty}^{\infty} (-1)^j e^{-2j^2x^2}}$

\paragraph{12. Построение доверительной области для теоретической функции распределения с помощью теоремы Колмогорова} ~\\
Теорема Колмогорова даёт возможность строить доверительные области для функции распределения. Пусть $\alpha$ - малое число, $x_\alpha$ - квантиль уровня $1-\alpha$, тогда
$$1-\alpha \approx P_F(\sqrt(n) \sup_x |F_n(x)-F(x)|<\alpha)={P_F(F_n(x)-\frac{x_\alpha}{\sqrt{n}} \leq F(x) \leq F_n(x)+\frac{x_\alpha}{\sqrt{n}})}$$

\paragraph{13. Что называется выборочной характеристикой случайной величины. Примеры (выборочные моменты, выборочные квантили)}
\begin{definition}[Выборочные характеристики] ~\\
	Пусть $\mathscr{F}$ - подмножество множество распределений $\alpha:\mathscr{F}\rightarrow\R$ - числовая характеристика. Тогда 
	\begin{align*}
		&\alpha(F) \text{ - теоретическая характеристика}\\
		&\alpha(F_n) \text{ - выборочная характеристика}\\
	\end{align*}
\end{definition}
\begin{example}[Выборочные моменты]~\\
	\begin{itemize}
		\item $\bar X = \cfrac{1}{n}\sum_{i=1}^nX_i$ - выборочное среднее.
		\item $s^2 = \cfrac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2$ - выборочная дисперсия.
		\item $\alpha_k = \cfrac{1}{n}\sum_{i=1}^nX_i^k$ - выборочные начальные моменты.
		\item $\beta_k = \cfrac{1}{n}\sum_{i=1}^n(X_i-\bar X)^k$ - выборочные центральные моменты.
	\end{itemize}
\end{example}
\begin{example}[Выборочные квантили] ~\\
	$\hat{\xi_p}$ - квантиль порядка $p$, $\hat{\xi_{p}}=X_{([np]+1)}$, если $np\notin\mathbb{Z}$, иначе $\hat{\xi_{p}}\in [X_{(np)};X_{(np+1)})$
\end{example}

\paragraph{14. Виды сходимости случайных величин и распределений} ~\\
\begin{definition}[Сходимость по вероятности] ~\\
	Мы говорим, что последовательность СВ $\{\xi_n\}$ сходится по вероятности к СВ $\xi$ при $n\rightarrow\infty$ ($\xi_n\overset{P}{\rightarrow}\xi$), если
	\[\forall \varepsilon > 0 : P(|\xi_n-\xi|\geq\varepsilon)\underset{n\rightarrow\infty}{\longrightarrow}0\]
\end{definition}
\begin{definition}[Сходимость почти наверное] ~\\
	Мы говорим, что последовательность СВ $\{\xi_n\}$ сходится почти наверное к СВ $\xi$ при $n\rightarrow\infty$ ($\xi_n \xrightarrow{\text{п.н.}} \xi$), если
	\[P\{\omega:\xi_n(\omega)\underset{n\rightarrow\infty}{\longrightarrow}\xi(\omega)\}=1\]
\end{definition}
\begin{definition}[Сходимость по распределению] ~\\
	Мы говорим, что последовательность СВ $\{\xi_n\}$ сходится по распределению (слабо) к СВ $\xi$ ($\xi_n\Rightarrow\xi$), если
	\[\underset{x}{\sup}|F_n(x) - F(x)|\underset{n\rightarrow\infty}{\longrightarrow}0\]
\end{definition}
\begin{definition}[Сходимость в степени p]
	Мы говорим, что последовательность СВ $\{\xi_n\}$ сходится в степени $p$ к СВ $\xi$ ($\xi_n\overset{p}{\rightarrow}\xi$), если
	\[\forall p > 0: \sE|\xi_n-\xi|^p\overset{n\rightarrow\infty}{\longrightarrow}0\]
\end{definition}
\paragraph{15. Метод построения доверительных эллипсоидов для параметров линейной регрессии} ~\\
\begin{definition}[Доверительный эллипсоид] ~\\
Пусть $\hat\Theta = \{\psi \mid (\hat\psi - \psi)^TB^{-1}(\hat\psi - \psi) \leq qs^2x_\alpha\}$\\
Где $x_\alpha = F^{-1}_{q,n-r}(1-\alpha)$, то есть $F_{1,n-r}(x_\alpha)=1-\alpha$\\
Тогда $P_\theta(\psi \in \hat\Theta) = 1-\alpha, \forall \theta \in \Theta$\\
Таким образом $\hat\Theta$ - доверительный эллипсоид уровня доверия $1-\alpha$
\end{definition}

\paragraph{16. Постановка задачи однофакторного дисперсионного анализа, F-критерий проверки однородности наблюдений в предположении нормальности.}

\paragraph{17. Построение Хи-квадрат, Стьюдента, Фишера-Снедекора распределений с помощью выборки из нормального закона}

\paragraph{18. Лемма Фишера}
\begin{theorem}[Фишера] ~\\
	Пусть $X_1,\dots,X_n$ - выборка из $N(a,\sigma^2)$. Тогда:
	\begin{enumerate}
		\item $\sqrt{n}\cfrac{\bar X - a}{\sigma} \ in N(0,1)$
		\item $\bar X$ и $s^2$ - независимые статистики
		\item $\cfrac{ns^2}{\sigma^2}$ имеет $\bigchi^2_{n-1}$ распределение
		\item $\sqrt{n-1}\cfrac{\bar X - a}{s}$ имеет $S_{n-1}$ распределение
	\end{enumerate}
\end{theorem}

\paragraph{19. Что такое функция потерь, риск}
\begin{definition}[Функция потерь] ~\\
	Пусть $\theta$ реальное значение параметра, тогда $W(\delta (\vec{X}),\theta)$ функция потерь, если
	\begin{itemize}
	 	\item $W(\delta (\vec{X}),\theta)>0,\forall \vec{X} \in \sX$
	 	\item $W(\theta,\theta)=0$
	 \end{itemize} 
\end{definition}
Используют различные функции потерь (в дальнейшем используем функцию Гаусса)
\begin{align}
	& W(\delta,\theta)=|\delta-\theta| \tag{Лаплас} \\
	& W(\delta,\theta)=(\delta-\theta)^2 \tag{Гаусс}
\end{align}
\begin{definition}[Риск]~\\
	Риском (функцией риска) называют $R(\delta,\theta) = \sE_\theta[W(\delta(\vec{X}),\theta)]$
\end{definition}

\paragraph{20. Какая оценка называется состоятельной}
\begin{definition}[Состоятельная оценка] ~\\
	 Точечная оценка $\delta^{(n)}(\vec{X})$ называется состоятельной, если
	 $$\delta^{(n)}(\vec{X}) \xrightarrow{p_\theta} \theta,\forall \theta \in \Theta $$
\end{definition}

\paragraph{21. Какая оценка называется несмещенной, что такое смещение}
\begin{definition}[Несмещенная оценка] ~\\
	Мы будем называть оценку $\delta(\vec X)$ функции параметра $g(\theta)$ несмещённой, если:
	\[\forall \theta \in \Theta: \sE_\theta\delta(\vec X) = g(\theta)\]
\end{definition}
\begin{definition}[Смещение оценки] ~\\
	Мы будем называть смещением оценки величину
	\[b_g(\theta) = \sE_\theta\delta(\vec X) - g(\theta)\]
\end{definition}

\paragraph{22. Какая оценка называется асимптотически нормальной}
\begin{definition}[Асимптотически нормальная оценка] ~\\
	Точечная оценка $\delta^{(n)}(\vec{X})$ называется асимптотически нормальной, если
	$$\sqrt{n} (\delta^{(n)}(\vec{X}) - \theta) \underset{P_\theta}{\Rightarrow} \mathcal{N}(0,\sigma^2(\theta))$$
\end{definition}

\paragraph{23. Определение несмещенной оценки с равномерно минимальной дисперсией}
\begin{definition}[НРМД оценка] ~\\
	Пусть $\delta_0(\vec X)$ - несмещённая оценка $g(\theta)$. Мы будем называть её несмещённой с равномерно минимальной дисперсией, если:
	\begin{gather*}		
		\forall \delta(\vec X) \text{ - несмещенных оценок } g(\theta):\\
		\sD\delta_0(\vec X) \leq \sD\delta(\vec X),\ \forall \theta \in \Theta
	\end{gather*}
\end{definition}

\paragraph{24. Как используются теорема Рао-Блэкуэлла-Колмогорова и теорема Лемана-Шеффе при построении НРМД-оценки}
\begin{statement}
	По теореме Рао-Блэкуэлла-Колмогорова, если $T$ - достаточная статистика семейства $\sP$, а $\delta$ - оценка параметра $\theta$ и $\eta=\sE(\delta|T)$:
	\[R(\delta,\theta)\geq R(\eta,\theta)\]
	Для несмещённой оценки $\delta$ параметра $\theta$
	\[R(\delta,\theta) = \sD(\delta, \theta)\]
	По теореме Лемана-Шеффе существует не более одной несмещённой оценки параметра $\theta$, являющейся функцией от полной достаточной статистики, то есть если $T$ - полная достаточная статистика, то
	\[\delta_0 = \sE(\delta|T) \text{ - НРМД оценка}\]
\end{statement}
\paragraph{25. Неравенство Гельдера}

\paragraph{26. Что представляет собой простая регрессионная модель}
\begin{statement}[Простая регрессионная модель]
	\[Y_i = \beta_1 + \beta_2 Z_i + \varepsilon_i,\ X = \begin{pmatrix}
		1   & \cdots & 1\\
		Z_1 & \cdots & Z_n\\
	\end{pmatrix}\]
\end{statement}

\paragraph{27. Что представляет собой полиномиальная регрессионная модель}
\begin{statement}[Полиномиальная модель]
	\[Y_i = \sum_{j=1}^s \beta_jZ_i^{j-1},\ X = \begin{pmatrix}
		1 & \cdots & 1\\
		Z_1 & \cdots & Z_n\\
		Z_1^2 & \cdots & Z_n^2\\
		\vdots & \ddots & \vdots\\
		Z_1^{s-1} & \cdots & Z_n^{s-1}
	\end{pmatrix}\]
\end{statement}

\paragraph{28. Связь между различными видами сходимости случайных величин и распределений}
\begin{lemma}[О видах сходимости] ~\\
	Дано: $\xi_n,\xi$
	\begin{enumerate}
		\item $\xi_n\xrightarrow{\text{п.н.}}\xi \implies \xi_n \overset{P}{\rightarrow}\xi$
		\item $\xi_n\underset{p}{\rightarrow}\xi \implies \xi_n \overset{P}{\rightarrow}\xi$
		\item $\xi_n \overset{P}{\rightarrow}\xi \implies \xi_n \Rightarrow \xi$
	\end{enumerate}
\end{lemma}

\paragraph{29. Метод максимального правдоподобия}
\begin{definition}[Функция правдоподобия] ~\\
	Пусть $X_1,\dots,X_n$ - набор независимых наблюдений с плотностями $f_{\theta,1},\dots,f_{\theta,n}$ соответственно. Мы будем называть функцией правдоподобия
	\[L(\vec X, \theta) = \prod_{i=1}^nf_{\theta,i}(X_i),\ \theta \in \Theta\]
	А значение этой функции при фиксированном $\theta$ правдоподобием наблюдений.
\end{definition}
\begin{definition}[Метод максимального правдоподобия] ~\\
	Идея: найти значение $\hat\theta(\vec X) \in \Theta$ такое, что
	\[\forall \theta \in \Theta: L(\vec X, \hat\theta(\vec X)) \leq L(\vec X, \theta)\]
\end{definition}

\paragraph{30. Определение достаточной статистики}
\begin{definition}[Достаточная статистика]
	Статистика $T$ назвается достаточной, если условное распределение $X$ при условии $T$ не зависит от параметра
	$$P_\theta(X\in A|T) = P_{X|T}(A),\forall \theta \in \Theta $$
\end{definition}

\paragraph{31. Теорема факторизации Неймана-Фишера}
\begin{theorem}[Неймана-Фишера]
	Статистика $T$ достаточна тогда и только тогда, когда функцию правдоподобия можно представить в виде
	\[L(\vec X, \theta) = g(T(\vec X),\theta)h(\vec X),\]
	где $g(x,\theta)$ и $h(x)$ - некоторые неотрицательные функции.
\end{theorem}

\paragraph{32. Какое известное двумерное распределение обладает свойством линейности регрессии (одной компоненты по другой)}

\paragraph{33. Каким свойством обладают однопараметрические семейства распределений для которых существует эффективная оценка}

\paragraph{34. Определение экспоненциального семейства}

\paragraph{35. Определение полной достаточной статистики и подчиненной статистики}
\begin{definition}[Полная достаточная статистика] ~\\
	Достаточная статистика $T$ называется полной, если выполнение условия $\forall \theta \in \Theta: \sE_\theta g(T) = 0$ влечет $g(T) = 0$  с вероятностью 1.
\end{definition}
\begin{definition}[Подчиненная статистика] ~\\
	Статистика $T$ называется подчиненной, если её распределение не зависит от параметра
	$$P_\theta (T\in A) = P_T(A)$$
\end{definition}

\paragraph{36. В чем состоит свойство асимптотической нормальности оценки максимального правдоподобия}

\paragraph{37. Что такое функция правдоподобия}
\begin{definition}[Функция правдоподобия] ~\\
	Пусть $X_1,\dots,X_n$ - набор независимых наблюдений с плотностями $f_{\theta,1},\dots,f_{\theta,n}$ соответственно. Мы будем называть функцией правдоподобия
	\[L(\vec X, \theta) = \prod_{i=1}^nf_{\theta,i}(X_i),\ \theta \in \Theta\]
	А значение этой функции при фиксированном $\theta$ правдоподобием наблюдений.
\end{definition}
Правдоподобие - это вероятность того, что теоретическое распределение приняло значения из выборки.
\paragraph{38. Что представляет собой семейство распределений Пуассона}
\begin{definition}[Распределение Пуассона] ~\\
	Рассмотрим последовательность биномиальных распределений $Bi(p,k),k \in \N, p = \lambda/k$. По теореме Пуассона эта последовательность сходится при $n\rightarrow\infty$ к распределению Пуассона
	\[P(\lambda) = \cfrac{\lambda^k}{k!}e^{-\lambda}\]
	Отметим, что если $X \sim P(\lambda_1)$ и $Y \sim P(\lambda_2)$ - НСВ, то $X + Y \sim P(\lambda_1 + \lambda_2)$
\end{definition}
\paragraph{39. Какие распределения называются показательными}

\paragraph{40. Определение регулярного эксперимента и информации Фишера}
\begin{definition}[Регулярный эксперимент] ~\\
	Пусть $(\sX,\sF,\sP), \sP = \{P_\theta, \theta \in \Theta\}$ - статистический эксперимент, $\Theta \subseteq \R$.\\
	Эксперимент называется регулярным, если выполняются следующие требования:
	\begin{enumerate}
		\item $\rho << \mu, p_\theta = \cfrac{\mathrm{d}\rho_\theta}{\mathrm{d}\mu}, \theta \in \Theta$\\
			  $p_\theta(\vec X)$ - непрерывно дифференцируема по $\theta$ (для почти всех $x$), т.е.
			  $\exists\, {p'}_\theta, (x) = \cfrac{\partial p_\theta(x)}{\partial\theta}$ - непрерывная функция
		\item $\cfrac{\partial}{\partial\theta}\int\limits_\sX p_\theta(x)\mathrm{d}\mu(x) = \int\limits_\sX \cfrac{\partial p_\theta(x)}{\partial\theta}\mathrm{d}\mu(x)$
		\item $\exists I(\theta): 0 < I(\theta) < \infty$, определяемое соотношением:
		\[I(\theta) = \int\limits_\sX \cfrac{p'(\vec X)^2}{p(\vec X)}\mathrm{d}\mu(\vec X)\]
	\end{enumerate}
	Или же в терминах функций правдоподобия:
	\begin{gather*}
		LL(\vec X, \theta) = \log L(\vec X, \theta)\\
		\cfrac{\partial LL(\vec X, \theta)}{\partial\theta} = \cfrac{\cfrac{\partial L(\vec X, \theta)}{\partial \theta}}{L(\vec X, \theta)} = \cfrac{{p'}_\theta(\vec X)}{p_\theta(\vec X)}
	\end{gather*}
	\begin{enumerate}
		\item $L(\vec X, \theta)$ - непрерывна и дифференцируема по $\theta$
		\item $\sE_\theta\cfrac{\partial LL(\vec X, \theta)}{\partial\theta} = 0,\ \forall \theta$
		\item С теми же условиями на $I(\theta)$:
		\[I(\theta) = \sE_\theta\bigg(\cfrac{\partial LL(\vec X, \theta)}{\partial \theta}\bigg)^2 = \sD_\theta\bigg(\cfrac{\partial LL(\vec X, \theta)}{\partial \theta}\bigg)\]
	\end{enumerate}
	При этом $I(\theta)$ - информация Фишера.
\end{definition}
\paragraph{41. Неравенство Рао-Крамера}
\begin{theorem}[Неравенство Рао-Крамера] ~\\
	Пусть $(\sX,\sF,\sP), \sP = \{P_\theta, \theta \in \Theta\}$ - статистический эксперимент, $I(\theta)$ - информация Фишера, $\delta$ - разрешённая оценка, $b$ - диффиренцируемое смещение $\theta$	$(b(\theta) = \sE_\theta\delta(\vec X)-\theta)$, тогда
	\[R_\delta(\theta)\geq\cfrac{(1+b'(\theta))^2}{I(\theta)}+b(\theta)^2\]
\end{theorem}
\begin{addition} ~\\
	\begin{enumerate}
		\item Если $\delta$ - несмещённая оценка, то
		\[R_\delta(\theta)\geq\cfrac{1}{I(\theta)}\]
		\item Пусть $X_1,\dots,X_n$ - выборка из распределения $F$, $I_1(\theta)$ - информация по $X_1$. Тогда
		\begin{gather*}
			I(\theta) = nI_1(\theta)\\
			\implies R_\delta(\theta)\geq\cfrac{1}{nI_1(\theta)}
		\end{gather*}
	\end{enumerate}
\end{addition}

\paragraph{42. Какая оценка называется эффективной по Фишеру}
\begin{definition}[R-эффективная оценка]
	Оценка $\delta$ называется R-эффективной (эффективной по Фишеру), если
	\begin{enumerate}
		\item $\delta$ - несмещённая оценка
		\item $R_\delta = \cfrac{1}{I(\theta)}$ (равенство в неравенстве Рао-Крамера).
	\end{enumerate}
\end{definition}

\paragraph{43. Центральная предельная теорема Леви}

\paragraph{44. Что называется доверительным интервалом уровня $\alpha$}
\begin{definition}[Доверительный интервал уровня $\alpha$] ~\\
	Интервал $[T_1,T_2]$, образованной парой статистик $T_1(\bar X)$ и $T_2(\bar X)$ называется доверительным интервалом с уровнем доверия $\alpha$, если
	\[\forall \theta \in \Theta: P_\theta(T_1 \leq \theta \leq T_2) \geq \alpha\]
\end{definition}

\paragraph{45. Законы больших чисел}

\paragraph{46. Понятие асимптотического доверительного интервала} ~\\
Дано: $(\sX_n,\sF_n,\sP_n), \sP_n=\{P_{n,\theta},\theta\in\Theta\}$
\begin{definition}[Асимптотический доверительный интервал]
	Интервал образованный парой статистик $T_{1n}(\vec X)$, $T_{2n}(\vec X)$ называется асимтотическим доверительным с уровнем доверия $1 - \alpha$, если
	\[\underset{n\rightarrow\infty}{\underline{\lim}}P_{n,\theta}(\theta \in [T_{1n},T_{2n}])\geq1-\alpha\]
\end{definition}


\paragraph{47. Метод построения асимптотических доверительных интервалов на базе асимптотически нормальной оценки} ~\\
Пусть $\delta(\vec X)$ - асимптотически нормальная оценка $\theta$, то есть $\sqrt{n}(\delta(\vec x)-\theta)\Rightarrow N(0, \sigma^2(\theta))$\\
Тогда:
\begin{gather*}
	\sqrt{n}\cfrac{\delta(\vec X) - \theta}{\sigma(\theta)} \Rightarrow N(0,1)\\
	G(\vec X, \theta) = \sqrt{n}\cfrac{\delta(\vec X)-\theta}{\sigma(\theta)}\\
\end{gather*}
Тогда если решение $G(\vec X,\theta) \in I_\alpha$ - интервал, то это доверительный интервал уровня доверия $1-\alpha$\\
Иначе пусть $\hat\sigma$ - состоятельная оценка $\sigma(\theta)$ (например $\hat\sigma=\sigma(\hat\theta)$, где $\hat\theta$ - состоятельная оценка $\theta$). Тогда:
\begin{gather*}
	\sqrt{n}\cfrac{(\delta(\vec X) - \theta)}{\hat\sigma(\vec X)} \Rightarrow N(0,1)\\
	\sqrt{n}\cfrac{(\delta(\vec X) - \theta)}{\hat\sigma(\vec X)} \in [-t_\alpha, t_\alpha]\\
	\iff \theta \in \bigg[\delta(\vec X) - \cfrac{t_\alpha\hat\sigma(\vec X)}{\sqrt{n}},\delta(\vec X) + \cfrac{t_\alpha\hat\sigma(\vec X)}{\sqrt{n}}\bigg]\text{ - АДИ уровня доверия } 1-\alpha
\end{gather*}

\paragraph{48. Постановка задачи ПСГ, верное решение и ошибки (таблица)} ~\\
Пусть $(\sX,\sF,\sP),\ \sP=\{P_\theta,\theta \in \Theta\}$ - статистический эксперимент.
\begin{definition}[Статистическая гипотеза] ~\\
	Статистической гипотезой называется утверждение о параметре $\theta$ вида
	\[H_i:\theta \in \Theta_H,\ \Theta_H \subseteq \Theta\]
\end{definition}
При проверке статистической гипотезы выдвигают:
\begin{align*}
	&H_0: \theta \in \Theta_0 \text{ - основную гипотезу}\\
	&H_A: \theta \in \Theta_A \text{ - альтернативную гипотезу}\\
	&\text{При этом } \Theta_0 \cap \Theta_1 = \varnothing\ (\Theta_0 \cup \Theta_1 = \Theta)
\end{align*}
\textbf{Задача:} выбрать $H_0$ или $H_1$ по результатам наблюдений. ~\\
\\
\textbf{Возможные варианты} ~\\
\\
\begin{tabular}{|c|c|c|c|}
	\hline
	$_\text{решение}\backslash^\text{положение дел}$ & Верна $H_0$ & Верна $H_A$ & Неверны обе \\
	\hline
	Принять $H_0$ & \textbf{+} & Ошибка II рода & \textbf{-} \\
	\hline
	Принять $H_A$ & Ошибка I рода & \textbf{+} & \textbf{-} \\
	\hline
\end{tabular}

\paragraph{49. Статистический критерий, доверительная, критическая области и область сомнений}
\begin{definition}[Статистический критерий] ~\\
	Критерий - правило выбора $H_0$ или $H_A$
	\[\varphi: \sX \rightarrow [0,1]\]
	$\varphi(\vec X)$ - вероятность отвергнуть $H_0$ по результатам наблюдений
\end{definition}
Для рандомизированного критерия выборочное пространство разбивается на три области:
\begin{enumerate}
	\item Доверительная (область принятия гипотезы) - $\{\vec x \in \sX: \varphi(\vec x) = 0\}$
	\item Критическая (область отвержения гипотезы) - $\{\vec x \in \sX: \varphi(\vec x) = 1\}$
	\item Область сомнения - $\{\vec x \in \sX: \varphi(\vec x) \in (0,1)\}$
\end{enumerate}
\begin{definition}[Нерандомизированный критерий] ~\\
	Критерий $\varphi: \sX \rightarrow \{0,1\}$ называется нерандомизированным критерием
\end{definition}

\paragraph{50. Выражение вероятностей ошибок в терминах критерия}
\begin{definition}[Ошибка I рода] ~\\
	Ошибка I рода - это отвержение $H_0$ при её справедливости
	\[P(\text{Ош.I рода}) = \sE_\theta\varphi(\vec x),\ \theta \in \Theta_0\]
\end{definition}
\begin{definition}[Ошибка второго рода] ~\\
	Ошибка II рода - это принятие $H_0$ при справделивости $H_1$
	\[P(\text{Ош.II рода}) = \sE_\theta(1-\varphi(\vec x)),\ \theta \in \Theta_A\]
\end{definition}

\paragraph{51. Лемма Неймана-Пирсона}
\begin{theorem}[Лемма Неймана-Пирсона] ~\\
	\begin{enumerate}
		\item Существует наиболее мощный критерий уровня значимости $\alpha$ проверки $H_0: \theta = \theta_0$ при $H_A: \theta = \theta_A$
		\item Данный критерий может быть построен следующим образом:
		\[\varphi(\vec x) = \begin{cases}
			0, &l(\vec x, \theta_A, \theta_0) < c_\alpha \\
			p, &l(\vec x, \theta_A, \theta_0) = c_\alpha \\
			1, &l(\vec x, \theta_A, \theta_0) > c_\alpha
		\end{cases}\]
		При этом значение $c_\alpha$ однозначно опредеятся из условия:
		\[\sE_{\theta_0}\varphi(\vec x) = P_{\theta_0}(l(\vec x, \theta_A, \theta_0) > c_\alpha) + pP_{\theta_0}(l(\vec x, \theta_A, \theta_0) = c_\alpha) = \alpha\]
		Если $P_{\theta_0}(l(\vec x, \theta_A, \theta_0) = c_\alpha) > 0$, то значение $p \in [0,1)$ определено однозначно.
		\item На множестве $l(\vec x, \theta_A, \theta_0) \neq c_\alpha$ НМ критерий определён однозначно
	\end{enumerate}
\end{theorem}

\paragraph{52. Проверка гипотез о параметрах линейной регрессионной модели (F-критерий)}

\paragraph{53. Критерий проверки гипотезы согласия о среднем нормального закона при неизвестной дисперсии} ~\\
\textbf{Гипотеза согласия}
\[H_0: a = a_0\ (\sigma^2\text{ неизвестна})\]
\textbf{Статистика}
\[-2LLR = n\ln\cfrac{s_0^2}{s^2} = n\ln\bigg(1 + \cfrac{(\vec X - a_0)^2}{s^2}\bigg)\]
\textbf{Критерий}
\[\varphi(\vec x) = \begin{cases}
	0, &LLR < x_\alpha\\
	1, &LLR > x_\alpha
\end{cases}\ x_\alpha: \mathscr{K}_1(x_\alpha) = 1 - \alpha\]
Где $\mathscr{K}_1$ - ф.р. $\bigchi_1^2$

\paragraph{54. Критерий проверки независимости хи-квадрат} ~\\
Пусть $(X_1,Y_1),\dots,(X_n,Y_n)$ - выборка из двухмерного распределения с функцией распределения $\mathbb{F}(x,y)$.
\textbf{Гипотеза независимости}
\[H_0: \mathbb{F}(x,y)=F_X(x)F_Y(y),\ \forall x,y; F_X(x) = \mathbb{F}(x,\infty), F_Y(y) = \mathbb{F}(\infty,y)\]
Пусть по $x$ - $r$ интервалов, по $y$ - $s$ интервалов.
\textbf{Статистика}
\[\bigchi^2(\theta)=\sum_{i=1}^r\sum_{j=1}^s\cfrac{\Big(\nu_{ij}-\cfrac{\nu_{i\cdot}\nu_{\cdot j}}{n}\Big)^2}{\cfrac{\nu_{i\cdot}\nu_{\cdot j}}{n}}\]
\textbf{Критерий}
\[\varphi(\vec {(x,y)}) = \begin{cases}
	0, \bigchi^2(\theta) \leq x_\alpha\\
	1, \bigchi^2(\theta) > x_\alpha
\end{cases}\ x_\alpha: \mathscr{K}_{(r-1)(s-1)}(x_\alpha) = 1 - \alpha\]
Где $\mathscr{K}_{(r-1)(s-1)}$ - ф.р. $\bigchi_{(r-1)(s-1)}^2$

\paragraph{55. Критерий проверки однородности двух независимых выборок из нормального закона с равными (неизвестными) дисперсиями}

\paragraph{56. Что такое статистика критерия и как она используется при построении нерандомизованного (асимптотического) критерия} ~\\
Статистика асимптотического критерия $G(\vec x)$ удовлетворяет условиям:
\begin{enumerate}
	\item $G_n(\vec x)$ при $n \rightarrow \infty$ не зависит от $\theta$ при $\theta \in \Theta_0$
	\item $G_n(\vec x)$ при $n \rightarrow \infty$ известно
	\item $G_n(\vec x)$ при $n \rightarrow \infty$ при $\theta \in \Theta_A$ отличается от распределения при $\theta \in \Theta_0$
\end{enumerate}
Нерандомизированный асимптотический критерий на базе статистики $G(\theta)$ имеет следующий вид
\[\varphi(\vec x) = \begin{cases}
	0, &G_n(\vec x) \in I_\alpha \\
	1, &G_n(\vec x) \notin I_\alpha
\end{cases}\ I_\alpha: P_\theta(G_n(\vec x) \in I_\alpha) \geq 1 - \alpha,\ \forall\theta\in\Theta_0\]

\paragraph{57. Критерий согласия Колмогорова} ~\\
Пусть $X_1,\dots,X_n$ - выборка из $F$. Тогда
\[H_0:F=F_0,\ F_0\text{ непрерывна - простая гипотеза согласия}\]
\textbf{Статистика критерия}
\[D_n = \sqrt{n}\sup_x|F_n(x)-F_0(x)|\]
\textbf{Асимптотический критерий}
\[\varphi(\vec x) = \begin{cases}
	0, &D_n \leq x_\alpha\\
	1, &D_n > x_\alpha
\end{cases}\]
Где $x_\alpha: K(x_\alpha) = 1 - \alpha$

\paragraph{58. Постановка задачи двухфакторного дисперсионного анализа, главные эффекты и взаимодействия}

\paragraph{59. Какое распределение называется мультиномиальным и его связь с хи-квадрат критерием} ~\\
Пусть $X_1,\dots,X_n$ - выборка из распределения, сконцентрированного в точках $\{1,\dots,s\}$ с атомами $p_1,\dots,p_s$.\\
Тогда распределение величин $(v_1,\dots,v_s), v_k=\sum_{i=1}^n\mathbbm{1}_{\{X_i=k\}}$ является мультиномиальным.\\
Плотность:
\[q_{\vec x}(\vec k) = P(x_1 = k_1,\dots,x_n=k_n)=\cfrac{n!}{k_1!\dots k_n!}p_1^{k_1}\dots p_n^{k_n}\mathbbm{1}_{\{\sum_{i=1}^nk_i=n\}}\]
В критерии хи-квадрат количества элементов в интервалах $n_1,\dots,n_N$ распределены мультиномиально.

\paragraph{60. Критерий хи-квадрат для проверки простой гипотезы согласия}
Статистика:
\[\bigchi^2 = \sum_{i=1}^r\cfrac{(\nu_i-np_{i0})^2}{np_{i0}}\]
Где $r$ - число интервалов.\\
Критерий:
\[\varphi(\vec x) = \begin{cases}
	0, &\bigchi^2 \leq x_\alpha\\
	1, &\bigchi^2 > x_\alpha
\end{cases}\ x_\alpha: \mathscr{K}_{r-1}(x_\alpha) = 1 - \alpha\]
Где $\mathscr{K}_{r-1}$ - ф.р. $\bigchi_{r-1}^2$

\paragraph{61. Критерий хи-квадрат для проверки сложной параметрической гипотезы согласия} ~\\
$H_0: p_i = p_i(\theta), \text{ при некоторых } \theta \in Theta \subset \R^d$\\
Аналог статистики:
\[\bigchi^2(\theta) = \sum_{i=1}^n\cfrac{(\nu_i-np_i(\theta))^2}{np_i(\theta)} \text{ - зависит от параметра}\]
Статистика:
\[\widetilde{\bigchi}^2 = \min_{\theta \in \Theta} \bigchi^2(\theta) \overset{H_0}{\Rightarrow}\bigchi_{r-1-d}^2\]
\[\varphi(\vec x) = \begin{cases}
	0, &\widetilde{\bigchi}^2 \leq x_\alpha\\
	1, &\widetilde{\bigchi}^2 > x_\alpha
\end{cases}\ x_\alpha: \mathscr{K}_{r-1-d}(x_\alpha) = 1 - \alpha\]
Где $\mathscr{K}_{r-1-d}$ - ф.р. $\bigchi_{r-1-d}^2$

\paragraph{62. Какой критерий называется наиболее мощным} ~\\
Тот для которого мощность критерия $\beta:\Theta_1\rightarrow[0,1],\ \beta(\theta) = \sE_\theta\varphi(\vec X) = 1 - P_\theta(\text{Ошибки II рода})$ максимальна при данном уровне значимости $\alpha$

\paragraph{63. Что определяет значение критерия и какой критерий называется нерандомизованным} ~\\
Наблюдения определяют значение критерия, так как критерий - статистика.\\
Нерандомизированным называется критерий $\varphi: \sX \rightarrow\{0,1\}$

\paragraph{64. Выражение ошибки 1-го рода и мощности в терминах критерия. В чем отличие между ними} ~\\
Ошибка первого рода $P_\theta(\text{Ошибки I рода}) = E_\theta\varphi(\vec x), \theta \in \Theta_0$\\
Мощность критерия $P\beta = 1 - P_\theta(\text{Ошибки II рода}) = E_\theta\varphi(\vec x), \theta \in \Theta_A$\\
Отличие в множестве параметров.

\paragraph{65. Что называется ошибкой I-го рода} ~\\
Ошибка I рода - это отвержение $H_0$ при её справедливости

\paragraph{66. Что называется ошибкой II-го рода и мощностью критерия} ~\\
Ошибка II рода - это принятие $H_0$ при справделивости $H_1$\\
Мощность критерия $P\beta = 1 - P_\theta(\text{Ошибки II рода}) = E_\theta\varphi(\vec x), \theta \in \Theta_A$

\paragraph{67. Что такое уровень значимости критерия, критическая и доверительная области} ~\\
При уровне значимости $\alpha$ $P_\theta(\text{Ошибки I рода}) \leq \alpha$
\begin{enumerate}
	\item Доверительная (область принятия гипотезы) - $\{\vec x \in \sX: \varphi(\vec x) = 0\}$
	\item Критическая (область отвержения гипотезы) - $\{\vec x \in \sX: \varphi(\vec x) = 1\}$
	\item Область сомнения - $\{\vec x \in \sX: \varphi(\vec x) \in (0,1)\}$
\end{enumerate}

\paragraph{68. Что такое распределение Стьюдента} ~\\
Плотность:
\[T(b,p)=\cfrac{\sqrt{2}\Gamma((p+1)/2)}{\Gamma(p/p)\sqrt{\pi p b}}\bigg(1+\cfrac{2x^2}{pb}\bigg)^{-(p+1)/2}\]

\paragraph{69. Характеристические функции: определение и основные свойства} ~\\
\begin{definition}[Характеристическая функция]
	\[\varphi_X(t)=\sE e^{itX} = \int\limits_{-\infty}^\infty e^{itx}P_X(dx)\]
	Называется характеристической функцией.\\
	При этом для дискретной с.в.
	\[\varphi_X(t)=\sum_{k=1}^\infty e^{itx_k}p_k\]
	А для непрерывной
	\[\varphi_X(t)=\int\limits_{-\infty}{\infty}e^{itx}f_X(x)\mathrm{d}x\]
\end{definition}
Свойства:
\begin{enumerate}
	\item Пусть $X, Y$ - с.в. и $\varphi_X(t)=\varphi_Y(t),\ \forall t$. Тогда $P_X = P_Y$
	\item $|\varphi_X(t)|\leq 1,\ \forall t \in \R$
	\item $\varphi_X(0) = 1$
	\item $\varphi_X$ - непрерывна.
	\item $\varphi_aX(t) = \varphi_X(at),\ \forall a \in \R$
	\item Пусть $X_1,\dots,X_n$ - НСВ, $S_n = \sum_{i=1}^nX_i$. Тогда
	\[\varphi_{S_n}(t)=\prod_{i=1}^n\varphi_{X_i}(t)\]
	\item $\forall t \in \R: \varphi_X(-t)=\bar\varphi_X(t)$
	\item Пусть $F$ - ФР, $\varphi$ - ХФ. Если $F$ непрерывна в $a$, $b$, то
	\[F(b)-F(a)=\cfrac{1}{2\pi}\lim_{T\rightarrow\inf}\int\limits_{-T}^{+T}\cfrac{e^{-ita}-e^{-itb}}{it}\varphi(t)\mathrm{d}t\]
\end{enumerate}

\paragraph{70. Что такое гамма распределение, его свойства} ~\\
Плотность:
\[\Gamma(b,p) = \cfrac{x^{p-1}\exp(-x/b)}{b^p\Gamma(p)}\mathbbm{1}_{[a,\infty)}(x)\]
Свойства:\\
Пусть $X \sim \Gamma(b,q_1)$, $Y \sim \Gamma(b,q_2)$. Тогда:
\[X + Y \sim \Gamma(b,q_1+q_2)\]
$\bigchi_n^2 = \Gamma(2,n/2)$\\
$E(a) = \Gamma(1/a,1)$

\paragraph{71. Что представляет собой двухпараметрическое семейство нормальных распределений} ~\\
Плотность:\\
% \[\sN(\vec a, R) = (2\pi)^{-n/2}|R|^{-1/2}\exp(-\cfrac{1}{2}(\vec x - \vec a)R^{-1}(\vec x - \vec a)^T)\]
\[\cfrac{1}{\sqrt{2\pi\sigma^2}}\exp\bigg(-\cfrac{(x-a)^2}{2\sigma^2}\bigg)\]


\paragraph{72. Построение доверительных интервалов для параметров линейной регрессионной модели в предположении нормальности} ~\\
\begin{gather*}
	\hat\psi \sim \sN(\psi,b\sigma^2),\ (b\sigma^2=\mathrm{D}\hat\psi)\\
	\cfrac{\hat\psi-\psi}{\sqrt{b}\sigma}\sim\sN(0,1),\ \cfrac{(n-r)s^2}{\sigma^2}\sim\bigchi_{n-r}^2\\
	\implies \cfrac{(\hat\psi-\psi)/\sqrt{b}\sigma}{\sqrt{\cfrac{(n-r)s^2}{(n-r)\sigma^2}}} = \cfrac{\hat\psi-\psi}{\sqrt{b}s} \sim S_{n-r}\\
	t_\alpha:S_{n-r}(t_\alpha) = 1 * \cfrac{\alpha}{2}
\end{gather*}
Тогда
\begin{gather*}
	P_\theta(-t_\alpha \leq \cfrac{\hat\psi-\psi}{\sqrt{b}s} \leq t_\alpha) = 1 - \alpha, \forall \theta\\
	P_\theta(\hat\psi - t_\alpha\sqrt{b}s \leq \psi \leq \hat\psi + t_\alpha\sqrt{b}s)\\
\end{gather*}
И $\hat\psi \pm t_\alpha\sqrt{b}s$ - доверительный интервал уровня доверия $1 - \alpha$

\end{document}
