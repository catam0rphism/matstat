\documentclass[titlepage]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[english, russian]{babel} % Localisation
% \usepackage{pscyr} % fonts

% \usepackage{fontspec} % loaded by polyglossia, but included here for transparency 
% \usepackage{polyglossia}
% \setmainlanguage{russian} 
% \setotherlanguage{english}

\usepackage{amssymb} % More math symbols
\usepackage{amsmath} % Math constructions
\usepackage{amsthm} % Theorems
\usepackage{subfiles} % File separation
\usepackage{titlepic} % Logo =)
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{mathtools} % multiline equations with curly brace
\usepackage{mathrsfs}

\newcommand{\sP}{\mathcal{P}}
\newcommand{\sF}{\mathfrak{F}}
\newcommand{\sX}{\mathfrak{X}}
\newcommand{\sE}{\mathrm{E}}
\newcommand{\sD}{\mathrm{D}}
\newcommand{\sN}{\mathrm{N}} % N as N(a,s^2)
\newcommand{\R}{\mathbb{R}} % R as in Real
\newcommand*{\bigchi}{\mbox{\Large$\chi$}} % Chi for chi-square
\DeclareMathOperator{\ex}{ex} % Example
\DeclareMathOperator{\proj}{Pr} % Projection

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{definition}{Определение}
\newtheorem*{example}{Пример}
\newtheorem*{statement}{Утверждение}
\newtheorem*{suggestion}{Предложение}

\begin{document}
\paragraph{1. Что такое регрессия величины Y по величине X и что представляет собой модель линейной регрессии} % (fold) 
\begin{definition}[Регрессия] ~\\
	Пусть $Y$ - наблюдение,  $Z$ - характеристика, определяющая распределение $Y$, $F_Z$ - распределение $Y$ при фиксированном $Z$.\\
	Пусть $Y_1,\dots,Y_n$ - независимы.
	Установим зависимость $Y_i$ от $i$.
	Сопоставим $\forall i: i \mapsto Z_i \implies F_i \equiv F_{Z_i}$.
	Обычно эту зависимость задают параметрически ($\ex: F_i = g_\theta(F_{Z_0}), \theta \in \R^d$).\\
	Тогда $E_\theta(Y|Z) = g_\theta(Z)$ - \textbf{регрессия} $Y$ по $Z$.\\
	\textbf{Модель линейной регрессии}
	\[\sE_\theta(Y|Z) = X^T\beta,\ \beta =
		\begin{pmatrix}
			\beta_1\\
			\vdots\\
			\beta_n
		\end{pmatrix}
	\]
	В условиях этой модели
	\begin{align*}
		\sE_\theta Y_i &= (X(Z_i))^T\beta\\
		Y_i &= (X(Z_i))^T\beta + \varepsilon_i\\
		Y &= X^T\beta + \varepsilon (\sE\varepsilon = 0)\\
	\end{align*}
	Где $X \in M_{m\times n}$ - матрица регрессоров, $\beta$ - $m \times 1$ - столбец параметров, $\varepsilon = (\varepsilon_1,\dots,\varepsilon_n)^T\text{ - вектор отклонений.}$
\end{definition}

\paragraph{2. Что представляет собой оценка по методу наименьших квадратов}
\begin{definition}[Метод наименьших квадратов]~\\
	Пусть $Y = X^T\beta + \varepsilon$.\\
	Рассмотрим $S(\beta) = ||Y-X^T\beta||^2=(Y-X^T\beta)^T(Y-X^T\beta) = \sum_{i=1}^n(Y_i-\sum_{j=1}^mx_{ji}\beta_j)^2$.\\
	Тогда $\hat \beta$ называется МНК-оценкой $\beta$, если $\forall \beta: S(\hat \beta)<=S(\beta)$.\\
\end{definition}

\paragraph{3. Геометрический смысл метода наименьших квадратов}
\[	Y = \sum_{i=1}^m X_i\beta_i + \varepsilon\text{, где }X_j = \begin{pmatrix}
	x_{j1}\\
	\vdots\\
	x_{jn}
\end{pmatrix}\text{ - транспонированные строки X.}\]
Пусть $Y \in V_n$ - линейное пространство наблюдений, $V_r = \alpha(X_1,\dots,X_m)$  - линейное подпространство на $(X_1,\dots,X_m)$, а $r = rk(x)$ - размерность пространства.\\
\begin{align*}
	\text{Тогда } &||Y-X^T\beta|| \text{- расстояние от $Y$ до точки  в $V_r$}\\
	&||Y-X^T\beta||\rightarrow \min\text{, если } \eta = X^T\beta=\proj_{V_r}Y\\
	\text{При этом } & X_k \perp (Y-X^T\beta), \forall k \in \{1\ldots m\}\\
	\iff & X_k^T(Y-X^T\beta) = 0, \forall k \in \{1\ldots m\}\\
	\iff & X(Y-X^T\beta) = 0\\
	\iff & XX^T\beta = XY\\
	\implies &\text{ существует единственное решение.}
\end{align*}

\paragraph{4. Какие функции параметра модели линейной регрессии допускают несмещенное оценивание}
\begin{definition}[Функция параметра] ~\\
	Мы будем называть $\psi$ функцией параметра $\beta$, если
	\[\psi = C^T\beta,\,(\psi = (\psi_1,\dots,\psi_q)^T)\]
\end{definition}
\begin{definition}[Линейная оценка] ~\\
	Мы будем называть $\hat \psi$ линейной оценкой, если
	\[\hat\psi = AY\ (A \in M_{q \times n})\]
\end{definition}
\begin{definition}[ДНО функция параметра] ~\\
	Мы будем называть функцию параметра $\psi = C^T\beta$ допускающей несмещённое оценивание (ДНО), если
	\[\exists \hat \psi = AY: \sE_\theta \hat \psi = \psi,\,\forall \theta \in \Theta = \R^m \times \R\]
\end{definition}

\paragraph{5. Теорема Гаусса-Маркова}
\begin{theorem}[Гаусса-Маркова] ~\\
	Пусть $\psi$ - ДНО функция параметра, $\psi = C^T\beta, q = 1$. Тогда существует $\hat\psi$ - линейная несмещённая оценка, такая что $\hat\psi$ - НРМД оценка.\\
	При этом для любых $\hat\beta$ решений системы нормальных уравнений $\hat\psi=C^T\hat\beta$
\end{theorem}

\paragraph{6. Что такое нормальные уравнения} ~\\
МНК-оценка $\hat \beta$ находится как решение системы нормальных уравнений
\[\begin{dcases}
		\frac{\partial S}{\partial \beta_k}\bigg(-2\sum_{i=1}^nx_{ki}(Y_i-\sum_{j=1}^mx_{ji}\beta_j)^2\bigg) = 0\\
		k \in (1,\dots,m)
\end{dcases}\]
\[\begin{dcases}
		\sum_{i=1}^nx_{ki}Y_i=\sum_{i=1}^nx_{ki}\sum_{j=1}^mx_{ji}\beta_j = 0\\
		k \in (1,\dots,m)
\end{dcases}\]
Или в матричной форме\\
$XX^T\beta=XY$ - система\\
$\hat\beta$ - решение системы, МНК-оценка\\
$|XX^T| \neq 0 \implies \hat\beta = (XX^T)^{-1}XY$, т.е. $\exists!$ решение.\\

\paragraph{7. Определение порядковых статистик и рангов}
\begin{definition}[Порядковые статистики, вариационный ряд и ранги] ~\\
	Достаточная статистика
	$$X_{(1)}\leq X_{(2)}\leq \dots X_{(n)}$$ называется вариационным рядом, $X_(k)$ - $k$-ая порядковая статистика. Ранг $R_k$ - номер $x_k$ в вариационном ряду
\end{definition}

\paragraph{8. Что такое выборочная функция распределения}
\begin{definition}[Выборочная функция распределения] ~\\
	Выборочной функцией распределения называют $$F_n(x)=\frac{1}{n}\sum_{i=1}^n\mathbbm{1}_{\{X_i < x\}}=\frac{\text{Число наблюдений меньших }x}{\text{общее число наблюдений}}$$
\end{definition}

\paragraph{9. Что представляет собой случайная величина, функция распределения которой – выборочная функция распределения}

\paragraph{10. Теорема Гливенко-Кантелли}
\begin{theorem}[Гливенко-Кантелли]
	$$\sup_x \{|F_n(x)-F(x)|\} \xrightarrow[n\rightarrow \infty]{P_\theta=1} 0,\forall F$$
\end{theorem}
Утверждает сильную состоятельность $F_n$

\paragraph{11. Теорема Колмогорова}
\begin{theorem}[Колмогорова]
	$$\sqrt{n} \sup_x \{F_n(x)-F(x)\} \underset{F}{\Rightarrow}\mathcal{K}$$
\end{theorem}
Где $\mathcal{K}$ -  распределение Колмогорова, функция распределения ${\mathcal{K}(x) = \sum_{j=-\infty}^{\infty} (-1)^j e^{-2j^2x^2}}$

\paragraph{12. Построение доверительной области для теоретической функции распределения с помощью теоремы Колмогорова} ~\\
Теорема Колмогорова даёт возможность строить доверительные области для функции распределения. Пусть $\alpha$ - малое число, $x_\alpha$ - квантиль уровня $1-\alpha$, тогда
$$1-\alpha \approx P_F(\sqrt(n) \sup_x |F_n(x)-F(x)|<\alpha)={P_F(F_n(x)-\frac{x_\alpha}{\sqrt{n}} \leq F(x) \leq F_n(x)+\frac{x_\alpha}{\sqrt{n}})}$$

\paragraph{13. Что называется выборочной характеристикой случайной величины. Примеры (выборочные моменты, выборочные квантили)}
\begin{definition}[Выборочные характеристики] ~\\
	Пусть $\mathscr{F}$ - подмножество множество распределений $\alpha:\mathscr{F}\rightarrow\R$ - числовая характеристика. Тогда 
	\begin{align*}
		&\alpha(F) \text{ - теоретическая характеристика}\\
		&\alpha(F_n) \text{ - выборочная характеристика}\\
	\end{align*}
\end{definition}
\begin{example}[Выборочные моменты]~\\
	\begin{itemize}
		\item $\bar X = \cfrac{1}{n}\sum_{i=1}^nX_i$ - выборочное среднее.
		\item $s^2 = \cfrac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2$ - выборочная дисперсия.
		\item $\alpha_k = \cfrac{1}{n}\sum_{i=1}^nX_i^k$ - выборочные начальные моменты.
		\item $\beta_k = \cfrac{1}{n}\sum_{i=1}^n(X_i-\bar X)^k$ - выборочные центральные моменты.
	\end{itemize}
\end{example}
\begin{example}[Выборочные квантили] ~\\
	$\hat{\xi_p}$ - квантиль порядка $p$, $\hat{\xi_{p}}=X_{([np]+1)}$, если $np\notin\mathbb{Z}$, иначе $\hat{\xi_{p}}\in [X_{(np)};X_{(np+1)})$
\end{example}

\paragraph{14. Виды сходимости случайных величин и распределений} ~\\
\begin{definition}[Сходимость по вероятности] ~\\
	Мы говорим, что последовательность СВ $\{\xi_n\}$ сходится по вероятности к СВ $\xi$ при $n\rightarrow\infty$ ($\xi_n\overset{P}{\rightarrow}\xi$), если
	\[\forall \varepsilon > 0 : P(|\xi_n-\xi|\geq\varepsilon)\underset{n\rightarrow\infty}{\longrightarrow}0\]
\end{definition}
\begin{definition}[Сходимость почти наверное] ~\\
	Мы говорим, что последовательность СВ $\{\xi_n\}$ сходится почти наверное к СВ $\xi$ при $n\rightarrow\infty$ ($\xi_n \xrightarrow{\text{п.н.}} \xi$), если
	\[P\{\omega:\xi_n(\omega)\underset{n\rightarrow\infty}{\longrightarrow}\xi(\omega)\}=1\]
\end{definition}
\begin{definition}[Сходимость по распределению] ~\\
	Мы говорим, что последовательность СВ $\{\xi_n\}$ сходится по распределению (слабо) к СВ $\xi$ ($\xi_n\Rightarrow\xi$), если
	\[\underset{x}{\sup}|F_n(x) - F(x)|\underset{n\rightarrow\infty}{\longrightarrow}0\]
\end{definition}
\begin{definition}[Сходимость в степени p]
	Мы говорим, что последовательность СВ $\{\xi_n\}$ сходится в степени $p$ к СВ $\xi$ ($\xi_n\overset{p}{\rightarrow}\xi$), если
	\[\forall p > 0: \sE|\xi_n-\xi|^p\overset{n\rightarrow\infty}{\longrightarrow}0\]
\end{definition}
\paragraph{15. Метод построения доверительных эллипсоидов для параметров линейной регрессии} ~\\
\begin{definition}[Доверительный эллипсоид] ~\\
Пусть $\hat\Theta = \{\psi \mid (\hat\psi - \psi)^TB^{-1}(\hat\psi - \psi) \leq qs^2x_\alpha\}$\\
Где $x_\alpha = F^{-1}_{q,n-r}(1-\alpha)$, то есть $F_{1,n-r}(x_\alpha)=1-\alpha$\\
Тогда $P_\theta(\psi \in \hat\Theta) = 1-\alpha, \forall \theta \in \Theta$\\
Таким образом $\hat\Theta$ - доверительный эллипсоид уровня доверия $1-\alpha$
\end{definition}

\paragraph{16. Постановка задачи однофакторного дисперсионного анализа, F-критерий проверки однородности наблюдений в предположении нормальности.}

\paragraph{17. Построение Хи-квадрат, Стьюдента, Фишера-Снедекора распределений с помощью выборки из нормального закона}

\paragraph{18. Лемма Фишера}
\begin{theorem}[Фишера] ~\\
	Пусть $X_1,\dots,X_n$ - выборка из $N(a,\sigma^2)$. Тогда:
	\begin{enumerate}
		\item $\sqrt{n}\cfrac{\bar X - a}{\sigma} \ in N(0,1)$
		\item $\bar X$ и $s^2$ - независимые статистики
		\item $\cfrac{ns^2}{\sigma^2}$ имеет $\bigchi^2_{n-1}$ распределение
		\item $\sqrt{n-1}\cfrac{\bar X - a}{s}$ имеет $S_{n-1}$ распределение
	\end{enumerate}
\end{theorem}

\paragraph{19. Что такое функция потерь, риск}
\begin{definition}[Функция потерь] ~\\
	Пусть $\theta$ реальное значение параметра, тогда $W(\delta (\vec{X}),\theta)$ функция потерь, если
	\begin{itemize}
	 	\item $W(\delta (\vec{X}),\theta)>0,\forall \vec{X} \in \sX$
	 	\item $W(\theta,\theta)=0$
	 \end{itemize} 
\end{definition}
Используют различные функции потерь (в дальнейшем используем функцию Гаусса)
\begin{align}
	& W(\delta,\theta)=|\delta-\theta| \tag{Лаплас} \\
	& W(\delta,\theta)=(\delta-\theta)^2 \tag{Гаусс}
\end{align}
\begin{definition}[Риск]~\\
	Риском (функцией риска) называют $R(\delta,\theta) = \sE_\theta[W(\delta(\vec{X}),\theta)]$
\end{definition}

\paragraph{20. Какая оценка называется состоятельной}
\begin{definition}[Состоятельная оценка] ~\\
	 Точечная оценка $\delta^{(n)}(\vec{X})$ называется состоятельной, если
	 $$\delta^{(n)}(\vec{X}) \xrightarrow{p_\theta} \theta,\forall \theta \in \Theta $$
\end{definition}

\paragraph{21. Какая оценка называется несмещенной, что такое смещение}
\begin{definition}[Несмещенная оценка] ~\\
	Мы будем называть оценку $\delta(\vec X)$ функции параметра $g(\theta)$ несмещённой, если:
	\[\forall \theta \in \Theta: \sE_\theta\delta(\vec X) = g(\theta)\]
\end{definition}
\begin{definition}[Смещение оценки] ~\\
	Мы будем называть смещением оценки величину
	\[b_g(\theta) = \sE_\theta\delta(\vec X) - g(\theta)\]
\end{definition}

\paragraph{22. Какая оценка называется асимптотически нормальной}
\begin{definition}[Асимптотически нормальная оценка] ~\\
	Точечная оценка $\delta^{(n)}(\vec{X})$ называется асимптотически нормальной, если
	$$\sqrt{n} (\delta^{(n)}(\vec{X}) - \theta) \underset{P_\theta}{\Rightarrow} \mathcal{N}(0,\sigma^2(\theta))$$
\end{definition}

\paragraph{23. Определение несмещенной оценки с равномерно минимальной дисперсией}
\begin{definition}[НРМД оценка] ~\\
	Пусть $\delta_0(\vec X)$ - несмещённая оценка $g(\theta)$. Мы будем называть её несмещённой с равномерно минимальной дисперсией, если:
	\begin{gather*}		
		\forall \delta(\vec X) \text{ - несмещенных оценок } g(\theta):\\
		\sD\delta_0(\vec X) \leq \sD\delta(\vec X),\ \forall \theta \in \Theta
	\end{gather*}
\end{definition}

\paragraph{24. Как используются теорема Рао-Блэкуэлла-Колмогорова и теорема Лемана-Шеффе при построении НРМД-оценки}
\begin{statement}
	По теореме Рао-Блэкуэлла-Колмогорова, если $T$ - достаточная статистика семейства $\sP$, а $\delta$ - оценка параметра $\theta$ и $\eta=\sE(\delta|T)$:
	\[R(\delta,\theta)\geq R(\eta,\theta)\]
	Для несмещённой оценки $\delta$ параметра $\theta$
	\[R(\delta,\theta) = \sD(\delta, \theta)\]
	По теореме Лемана-Шеффе существует не более одной несмещённой оценки параметра $\theta$, являющейся функцией от полной достаточной статистики, то есть если $T$ - полная достаточная статистика, то
	\[\delta_0 = \sE(\delta|T) \text{ - НРМД оценка}\]
\end{statement}
\paragraph{25. Неравенство Гельдера}

\paragraph{26. Что представляет собой простая регрессионная модель}
\begin{statement}[Простая регрессионная модель]
	\[Y_i = \beta_1 + \beta_2 Z_i + \varepsilon_i,\ X = \begin{pmatrix}
		1   & \cdots & 1\\
		Z_1 & \cdots & Z_n\\
	\end{pmatrix}\]
\end{statement}

\paragraph{27. Что представляет собой полиномиальная регрессионная модель}
\begin{statement}[Полиномиальная модель]
	\[Y_i = \sum_{j=1}^s \beta_jZ_i^{j-1},\ X = \begin{pmatrix}
		1 & \cdots & 1\\
		Z_1 & \cdots & Z_n\\
		Z_1^2 & \cdots & Z_n^2\\
		\vdots & \ddots & \vdots\\
		Z_1^{s-1} & \cdots & Z_n^{s-1}
	\end{pmatrix}\]
\end{statement}

\paragraph{28. Связь между различными видами сходимости случайных величин и распределений}
\begin{lemma}[О видах сходимости] ~\\
	Дано: $\xi_n,\xi$
	\begin{enumerate}
		\item $\xi_n\xrightarrow{\text{п.н.}}\xi \implies \xi_n \overset{P}{\rightarrow}\xi$
		\item $\xi_n\underset{p}{\rightarrow}\xi \implies \xi_n \overset{P}{\rightarrow}\xi$
		\item $\xi_n \overset{P}{\rightarrow}\xi \implies \xi_n \Rightarrow \xi$
	\end{enumerate}
\end{lemma}

\paragraph{29. Метод максимального правдоподобия}
\begin{definition}[Функция правдоподобия] ~\\
	Пусть $X_1,\dots,X_n$ - набор независимых наблюдений с плотностями $f_{\theta,1},\dots,f_{\theta,n}$ соответственно. Мы будем называть функцией правдоподобия
	\[L(\vec X, \theta) = \prod_{i=1}^nf_{\theta,i}(X_i),\ \theta \in \Theta\]
	А значение этой функции при фиксированном $\theta$ правдоподобием наблюдений.
\end{definition}
\begin{definition}[Метод максимального правдоподобия] ~\\
	Идея: найти значение $\hat\theta(\vec X) \in \Theta$ такое, что
	\[\forall \theta \in \Theta: L(\vec X, \hat\theta(\vec X)) \leq L(\vec X, \theta)\]
\end{definition}

\paragraph{30. Определение достаточной статистики}
\begin{definition}[Достаточная статистика]
	Статистика $T$ назвается достаточной, если условное распределение $X$ при условии $T$ не зависит от параметра
	$$P_\theta(X\in A|T) = P_{X|T}(A),\forall \theta \in \Theta $$
\end{definition}

\paragraph{31. Теорема факторизации Неймана-Фишера}
\begin{theorem}[Неймана-Фишера]
	Статистика $T$ достаточна тогда и только тогда, когда функцию правдоподобия можно представить в виде
	\[L(\vec X, \theta) = g(T(\vec X),\theta)h(\vec X),\]
	где $g(x,\theta)$ и $h(x)$ - некоторые неотрицательные функции.
\end{theorem}

\paragraph{32. Какое известное двумерное распределение обладает свойством линейности регрессии (одной компоненты по другой)}

\paragraph{33. Каким свойством обладают однопараметрические семейства распределений для которых существует эффективная оценка}

\paragraph{34. Определение экспоненциального семейства}

\paragraph{35. Определение полной достаточной статистики и подчиненной статистики}
\begin{definition}[Подчиненная статистика] ~\\
	Статистика $T$ называется подчиненной, если её распределение не зависит от параметра
	$$P_\theta (T\in A) = P_T(A)$$
\end{definition}

\paragraph{36. В чем состоит свойство асимптотической нормальности оценки максимального правдоподобия}

\paragraph{37. Что такое функция правдоподобия}

\paragraph{38. Что представляет собой семейство распределений Пуассона}

\paragraph{39. Какие распределения называются показательными}

\paragraph{40. Определение регулярного эксперимента и информации Фишера}

\paragraph{41. Неравенство Рао-Крамера}

\paragraph{42. Какая оценка называется эффективной по Фишеру}

\paragraph{43. Центральная предельная теорема Леви}

\paragraph{44. Что называется доверительным интервалом уровня $\alpha$}
\begin{definition}[Доверительный интервал уровня $\alpha$] ~\\
Интервал $[T_1,T_2]$, образованной парой статистик $T_1(\bar X)$ и $T_2(\bar X)$ называется доверительным интервалом с уровнем доверия $\alpha$, если
\[\forall \theta \in \Theta: P_\theta(T_1 \leq \theta \leq T_2) \geq \alpha\]
	
\end{definition}

\paragraph{45. Законы больших чисел}

\paragraph{46. Понятие асимптотического доверительного интервала}

\paragraph{47. Метод построения асимптотических доверительных интервалов на базе асимптотически нормальной оценки}

\paragraph{48. Постановка задачи ПСГ, верное решение и ошибки (таблица)}

\paragraph{49. Статистический критерий, доверительная, критическая области и область сомнений}

\paragraph{50. Выражение вероятностей ошибок в терминах критерия}

\par**agraph{51. Лемма Неймана-Пирсона}

\paragraph{52. Проверка гипотез о параметрах линейной регрессионной модели (F-критерий)}

\paragraph{53. Критерий проверки гипотезы согласия о среднем нормального закона при неизвестной дисперсии}

\paragraph{54. Критерий проверки независимости хи-квадрат}

\paragraph{55. Критерий проверки однородности двух независимых выборок из нормального закона с равными (неизвестными) дисперсиями}

\paragraph{56. Что такое статистика критерия и как она используется при построении нерандомизованного (асимптотического) критерия}

\paragraph{57. Критерий согласия Колмогорова}

\paragraph{58. Постановка задачи двухфакторного дисперсионного анализа, главные эффекты и взаимодействия}

\paragraph{59. Какое распределение называется мультиномиальным и его связь с хи-квадрат критерием}

\paragraph{60. Критерий хи-квадрат для проверки простой гипотезы согласия}

\paragraph{61. Критерий хи-квадрат для проверки сложной параметрической гипотезы согласия}

\paragraph{62. Какой критерий называется наиболее мощным}

\paragraph{63. Что определяет значение критерия и какой критерий называется нерандомизованным}

\paragraph{64. Выражение ошибки 1-го рода и мощности в терминах критерия. В чем отличие между ними}

\paragraph{65. Что называется ошибкой I-го рода}

\paragraph{66. Что называется ошибкой II-го рода и мощностью критерия}

\paragraph{67. Что такое уровень значимости критерия, критическая и доверительная области}

\paragraph{68. Что такое распределение Стьюдента}

\paragraph{69. Характеристические функции: определение и основные свойства}

\paragraph{70. Что такое гамма распределение, его свойства}

\paragraph{71. Что представляет собой двухпараметрическое семейство нормальных распределений}

\paragraph{72. Построение доверительных интервалов для параметров линейной регрессионной модели в предположении нормальности}

\end{document}
